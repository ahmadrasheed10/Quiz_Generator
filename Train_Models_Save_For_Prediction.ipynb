{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Classification Model Training - Google Colab Ready\n",
    "## Trains Random Forest, SVM, LSTM, CNN and saves them for prediction app\n",
    "\n",
    "**STEP 1**: Upload your `quiz_data.csv` file when prompted below!\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Handles file upload (for Colab)\n",
    "2. ‚úÖ Trains all 4 models (Random Forest, SVM, LSTM, CNN)\n",
    "3. ‚úÖ Evaluates them (calculates accuracy, precision, recall, etc.)\n",
    "4. ‚úÖ **SAVES the models** so prediction app can use them\n",
    "5. ‚úÖ Creates downloadable ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q scikit-learn torch pandas numpy matplotlib seaborn joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ UPLOAD YOUR DATA FILE\n",
    "\n",
    "**For Colab**: Click the button below to upload `quiz_data.csv`\n",
    "\n",
    "**For Local**: Make sure `quiz_data.csv` is in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file (Colab) or use local file\n",
    "if IN_COLAB:\n",
    "    print(\"üì§ Please upload your quiz_data.csv file:\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Get the uploaded filename\n",
    "    CSV_PATH = list(uploaded.keys())[0]\n",
    "    print(f\"‚úÖ Uploaded: {CSV_PATH}\")\n",
    "else:\n",
    "    CSV_PATH = 'quiz_data.csv'\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        print(f\"‚úÖ Found: {CSV_PATH}\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found: {CSV_PATH}\")\n",
    "        print(\"Please make sure quiz_data.csv is in the same directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TARGET_COLUMN = 'subject'  # Change to 'difficulty' or 'question_type' for other tasks\n",
    "SAVE_DIR = f'./saved_models/{TARGET_COLUMN}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìä Target: {TARGET_COLUMN}\")\n",
    "print(f\"üíæ Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} rows\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna(subset=['question', TARGET_COLUMN])\n",
    "df['question'] = df['question'].astype(str)\n",
    "\n",
    "print(f\"\\n‚úÖ After cleaning: {len(df)} samples\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[TARGET_COLUMN].value_counts())\n",
    "\n",
    "# Prepare X, y\n",
    "X = df['question'].values\n",
    "y = df[TARGET_COLUMN].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, f'{SAVE_DIR}/label_encoder.pkl')\n",
    "print(f\"\\n‚úÖ Label encoder saved!\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Random Forest (ML Model #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üå≤ TRAINING RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "print(\"\\n1. Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, f'{SAVE_DIR}/tfidf_vectorizer.pkl')\n",
    "print(\"   ‚úÖ TF-IDF vectorizer saved!\")\n",
    "print(f\"   Feature dimensions: {X_train_tfidf.shape[1]}\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\n2. Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n3. Evaluating...\")\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"   ‚úÖ Test Accuracy: {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, f'{SAVE_DIR}/random_forest.pkl')\n",
    "print(\"   ‚úÖ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train SVM (ML Model #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ TRAINING SVM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train LinearSVM\n",
    "print(\"\\n1. Training LinearSVM...\")\n",
    "linear_svm = LinearSVC(C=1.0, random_state=42, max_iter=2000, verbose=1)\n",
    "linear_svm.fit(X_train_tfidf, y_train)\n",
    "print(\"   ‚úÖ Training complete!\")\n",
    "\n",
    "# Calibrate for probabilities\n",
    "print(\"\\n2. Calibrating for probability predictions...\")\n",
    "svm_model = CalibratedClassifierCV(linear_svm, method='sigmoid', cv=3)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "print(\"   ‚úÖ Calibration complete!\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n3. Evaluating...\")\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"   ‚úÖ Test Accuracy: {accuracy_svm:.4f} ({accuracy_svm*100:.2f}%)\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(svm_model, f'{SAVE_DIR}/svm.pkl')\n",
    "print(\"   ‚úÖ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data for Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî§ PREPARING DATA FOR DEEP LEARNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"\\n1. Building vocabulary...\")\n",
    "word_counts = Counter()\n",
    "for text in X_train:\n",
    "    words = str(text).lower().split()\n",
    "    word_counts.update(words)\n",
    "\n",
    "# Create word to index mapping\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for idx, (word, count) in enumerate(word_counts.most_common(10000)):\n",
    "    vocab[word] = idx + 2\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"   ‚úÖ Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open(f'{SAVE_DIR}/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"   ‚úÖ Vocabulary saved!\")\n",
    "\n",
    "# Text to sequence function\n",
    "def text_to_sequence(text, max_length=512):\n",
    "    words = str(text).lower().split()\n",
    "    seq = [vocab.get(word, vocab['<UNK>']) for word in words[:max_length]]\n",
    "    seq = seq + [vocab['<PAD>']] * (max_length - len(seq))\n",
    "    return seq\n",
    "\n",
    "# Convert to sequences\n",
    "print(\"\\n2. Converting texts to sequences...\")\n",
    "X_train_seq = np.array([text_to_sequence(text) for text in tqdm(X_train, desc=\"Train\")])\n",
    "X_val_seq = np.array([text_to_sequence(text) for text in tqdm(X_val, desc=\"Val\")])\n",
    "X_test_seq = np.array([text_to_sequence(text) for text in tqdm(X_test, desc=\"Test\")])\n",
    "\n",
    "print(f\"   ‚úÖ Sequence shape: {X_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = TextDataset(X_train_seq, y_train)\n",
    "val_dataset = TextDataset(X_val_seq, y_val)\n",
    "test_dataset = TextDataset(X_test_seq, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ Dataloaders created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=2, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "# CNN Model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout=0.3):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        cat = self.dropout(cat)\n",
    "        output = self.fc(cat)\n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train LSTM (DL Model #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ TRAINING LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(label_encoder.classes_)\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=128,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = correct / total\n",
    "    print(f\"  Loss: {total_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "lstm_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        outputs = lstm_model(sequences)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_lstm = correct / total\n",
    "print(f\"‚úÖ Test Accuracy: {accuracy_lstm:.4f} ({accuracy_lstm*100:.2f}%)\")\n",
    "\n",
    "# Save model\n",
    "torch.save(lstm_model.state_dict(), f'{SAVE_DIR}/lstm_model.pt')\n",
    "lstm_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 128,\n",
    "    'num_classes': num_classes,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "with open(f'{SAVE_DIR}/lstm_config.pkl', 'wb') as f:\n",
    "    pickle.dump(lstm_config, f)\n",
    "print(\"‚úÖ LSTM model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train CNN (DL Model #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß† TRAINING CNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize model\n",
    "cnn_model = CNNClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=300,\n",
    "    num_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = correct / total\n",
    "    print(f\"  Loss: {total_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        outputs = cnn_model(sequences)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy_cnn = correct / total\n",
    "print(f\"‚úÖ Test Accuracy: {accuracy_cnn:.4f} ({accuracy_cnn*100:.2f}%)\")\n",
    "\n",
    "# Save model\n",
    "torch.save(cnn_model.state_dict(), f'{SAVE_DIR}/cnn_model.pt')\n",
    "cnn_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': 300,\n",
    "    'num_filters': 100,\n",
    "    'filter_sizes': [3, 4, 5],\n",
    "    'num_classes': num_classes,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "with open(f'{SAVE_DIR}/cnn_config.pkl', 'wb') as f:\n",
    "    pickle.dump(cnn_config, f)\n",
    "print(\"‚úÖ CNN model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä MODEL ACCURACIES:\")\n",
    "print(f\"  üå≤ Random Forest:  {accuracy_rf*100:.2f}%\")\n",
    "print(f\"  üéØ SVM:            {accuracy_svm*100:.2f}%\")\n",
    "print(f\"  üîÑ LSTM:           {accuracy_lstm*100:.2f}%\")\n",
    "print(f\"  üß† CNN:            {accuracy_cnn*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüíæ All models saved to: {SAVE_DIR}\")\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "for file in sorted(os.listdir(SAVE_DIR)):\n",
    "    size_mb = os.path.getsize(os.path.join(SAVE_DIR, file)) / (1024 * 1024)\n",
    "    print(f\"  ‚úÖ {file:30} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies\n",
    "models = ['Random Forest', 'SVM', 'LSTM', 'CNN']\n",
    "accuracies = [accuracy_rf*100, accuracy_svm*100, accuracy_lstm*100, accuracy_cnn*100]\n",
    "colors = ['#FF6B6B', '#FF8E53', '#4ECDC4', '#44A08D']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "plt.ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "plt.title(f'Model Comparison - {TARGET_COLUMN.upper()} Classification', fontsize=16, fontweight='bold')\n",
    "plt.ylim(min(accuracies) - 5, 100)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison chart saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download Models (For Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"üì¶ Creating ZIP file for download...\")\n",
    "    !zip -r saved_models.zip ./saved_models/\n",
    "    \n",
    "    print(\"\\n‚úÖ ZIP file created!\")\n",
    "    print(\"üì• Click below to download:\")\n",
    "    \n",
    "    from google.colab import files\n",
    "    files.download('saved_models.zip')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìù NEXT STEPS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"1. Extract saved_models.zip in your project folder\")\n",
    "    print(\"2. Run: streamlit run prediction_app.py\")\n",
    "    print(\"3. Start making predictions!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚úÖ Models saved locally!\")\n",
    "    print(\"\\nYou can now run: streamlit run prediction_app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
